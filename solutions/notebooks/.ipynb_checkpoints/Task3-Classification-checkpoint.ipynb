{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charged-democracy",
   "metadata": {},
   "source": [
    "# Task 3 : Dimensionality Reduction\n",
    "\n",
    "This notebook contains the process with which the classifiers were built for this task.\n",
    "It is long and detailed. Refer to the Short version of this notebook for a simpler, direct solution to the task at hand.\n",
    "\n",
    "# TOC\n",
    "* [Preface](#pre)\n",
    "* [Generator](#gen)\n",
    "* [Visualising the dataset](#vis)\n",
    "* [Classification](#class)\n",
    "    - [Logistic Regression](#lr)\n",
    "    - [K-Nearest Neighbours](#knn)\n",
    "    - [Decision Trees](#dt)\n",
    "    - [Random Forest](#rf)\n",
    "    - [RBF Support Vector Machine](#rbf)\n",
    "    - [Naive Bayes Classifier](#nb)\n",
    "    - [Artificial Neural Network](#ann)\n",
    "    - [AdaBoost](#ada)\n",
    "* [Concluding thoughts](#conc)\n",
    "\n",
    "# **Preface** <a class=\"anchor\" id=\"pre\"></a>\n",
    "\n",
    "Initially, I considered using scikit-learn to run all the above classifiers with. But after extracting the dataset, I realised that I don't have enough memory to load the entire dataset. This is a problem, because all sklearn functions require the entire dataset to be loaded into memory. Hence, in an attempt to circumvent this issue, I ended up implementing my own generator to yield the data one line at a time, and also wrote my own classifiers to work with this method. I also used a custom plotting function to generate the ROC.\n",
    "\n",
    "This is just a starting point, though. There are a couple of issues with the method I've adopted. For one, it loads elements into memory sequentially. This is a problem wherever we use batches, because we cannot gurantee there being a good, uniform split of elements from each class in the batch. This might lead to accuracy issues. But it can be mitigated by performing random access on the file, by maybe using seek() to read a specific line in the file, and then randomly reading the lines to make the training set. This might ensure uniformity across batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hindu-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-actress",
   "metadata": {},
   "source": [
    "# **Generator** <a class=\"anchor\" id=\"gen\"></a>\n",
    "\n",
    "As was mentioned above, I don't have enough memory to load the whole data set. So, the logical thing to do is to load in each row at a time.\n",
    "\n",
    "We load the file in the `getRow` generator, which in turn yields us one row from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator, because I don't have enough memory to load the dataset fully\n",
    "\n",
    "# Converts the read line into a list for further processing\n",
    "def processLine(l):\n",
    "    '''\n",
    "    Input:\n",
    "        l : line to convert into array\n",
    "        \n",
    "    Output:\n",
    "        l : numpy array, after conversion\n",
    "    '''\n",
    "    l = l.split(',')\n",
    "    l = [float(i) for i in l]\n",
    "    l = np.array(l)\n",
    "    return l\n",
    "\n",
    "# Generator to yield each row at a time\n",
    "def getRow():\n",
    "    with open('../../HIGGS_6M.csv', 'r') as fh:\n",
    "        while True:\n",
    "            line = fh.readline()\n",
    "            # print(line)\n",
    "            row = processLine(line)\n",
    "            # print(row)\n",
    "            yield row\n",
    "            \n",
    "def getBatch(SIZE):\n",
    "    x = getRow()\n",
    "    i = 0\n",
    "    batch = np.array([])\n",
    "    while i < SIZE:\n",
    "        row = next(x)\n",
    "        np.append(batch, row)\n",
    "    yield batch\n",
    "            \n",
    "# Test generator to see if it works\n",
    "x = getRow()\n",
    "print(next(x))\n",
    "print(next(x))\n",
    "# Deleting the generator, because we don't want any memory leaks\n",
    "del x\n",
    "\n",
    "# Test batch generator to see if it works\n",
    "x = getBatch(10)\n",
    "print(next(x))\n",
    "print(next(x))\n",
    "# Delete generator\n",
    "del x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
